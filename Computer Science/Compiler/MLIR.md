## MLIR이란?
- Multi Level Intermediate Representation
- "여러 수준의 추상화(IR)"을 하나의 통합 프레임워크 내에서 표현하도록 설계된 **프로그래머블 IR 인프라**
- Google의 TensorFlow 팀(특히 **Chris Lattner** (LLVM 창시자))가 주도하여 개발하여, 2019년 3월에 처음 공객됨
- **딥러닝 컴파일러**와 **도메인 특화 언어(DSL)**의 발전과 깊은 연관이 있음
- Multi Level?
	- "**계층적 추상화 레벨마다 알맞은 IR 표현을 갖는다.**"는 의미

| Level          | 대표 Dialect                     | 의미                                         |
| -------------- | ------------------------------ | ------------------------------------------ |
| **High-Level** | `tensor`, `mhlo`, `torch`      | 텐서 연산 수준에서의 모델 표현 (e.g., `add`, `softmax`) |
| **Mid-Level**  | `linalg`, `tosa`               | 커널 수준에서의 행렬 곱, convolution 등               |
| **Loop-Level** | `affine`, `scf`, `memref`      | 루프나 조건문 등 명령형 구조로 바뀐 수준                    |
| **Low-Level**  | `llvm`, `gpu`, `nvvm`, `spirv` | 메모리, 레지스터, 스레드 단위의 명령                      |

### MLIR 구조

[ Tensor Dialect ] ← 고수준 추상화 (모델 그래프)
      ↓
[ Linalg / MHLO / Torch Dialect ] ← 계산 커널 추상화
      ↓
[ Affine / SCF Dialect ] ← 루프와 제어 흐름 기반의 낮은 수준
      ↓
[ LLVM Dialect ] ← LLVM IR에 가까운 아주 저수준 IR
      ↓
[ LLVM-IR / PTX / XLA / GPU IR ] ← 머신 코드 타겟 백엔드

- 각 단계는 하나의 **Dialect (방언)** 으로 구현돼 있음
- ex)`linalg.matmul`이나 `tensor.extract`, `affine.for` 같은 연산이 각각 특정 Dialect에 속함

## 🧭 왜 등장했는가?

기존의 컴파일러 인프라(예: LLVM IR)는 **하드웨어 독립적인 표현**에 강하지만, 다음과 같은 한계를 가지고 있었기 때문

### 1. **도메인 특화 연산 표현 부족**

- 예를 들어 딥러닝에서는 `matmul`, `convolution`, `softmax` 같은 고수준 연산이 중요함
- 하지만 LLVM IR은 이런 연산을 표현할 수단이 없고, 낮은 수준의 연산(load/store/add)으로만 표현됨
- 결과적으로 **연산의 의미를 잃게 되어 최적화가 어렵고**, **비효율적인 코드 생성**이 많았음

### 2. **다양한 수준의 중간 표현이 필요함**

- 예: 고수준 (텐서 연산) → 중간 수준 (루프 및 버퍼) → 저수준 (레지스터와 포인터)
- 기존 IR은 단일 수준만을 가정하므로 **연산 수준 간 정보 전달**이 어려움

### 3. **다양한 하드웨어 대응 어려움**

- GPU, TPU, FPGA 등 다양한 백엔드에 맞춰 코드를 생성하려면 IR 자체도 유연해야 함
- 기존에는 백엔드별로 따로 DSL/컴파일러를 만들어야 했고, **중복 코드와 생태계 분산**이 심각했음

### 4. **재사용 가능한 컴파일러 인프라 부족**

- DSL마다 전용 컴파일러를 만들다 보면, **IR, 최적화, lowering 로직이 중복**됨
- MLIR은 **공통 인프라**를 제공하여, DSL들이 **공통 언어 위에서 동작**하게 하려는 목표를 가짐

## MLIR 특징

|특징|설명|
|---|---|
|✅ Dialect 기반|각 도메인/레벨마다 독립된 IR 표현 (예: `linalg`, `tensor`, `arith`)|
|✅ 멀티 레벨|고수준 텐서 연산부터 낮은 수준의 메모리 연산까지 표현 가능|
|✅ 재사용성|패스(pass), 변환(transform), 인프라를 재사용 가능하게 설계|
|✅ 확장성|새로운 연산이나 타입을 쉽게 추가할 수 있음|
|✅ 메타컴파일러|"IR을 위한 프레임워크" — DSL 컴파일러를 만들기 위한 메타도구|

## MLIR 요약

- **AI 모델 아키텍처 연산들은 너무 고수준이고 도메인 특화**되어 있어서,  
- **LLVM으로 직접 lowering하면 의미가 사라지고 최적화도 어렵다**  
- 그래서 **MLIR이라는 계층적 중간 표현(Meta-IR)**을 통해  
- **고수준 의미를 유지한 채로 점진적으로 저수준 IR로 변환**하여  
- **최적화 + 하드웨어 백엔드 타겟팅**까지 유연하게 처리하는 것.

## 연구의 목표
- MLIR까지만 설계를 진행하면, 그 이후에는 LLVM에서 제공하는 풍부한 최적화들로 최적화가 진행되므로, MLIR까지만 설계를 진행하여 LLVM IR까지만 생성해주면 될 듯!
## 🧭 왜 등장했는가?

기존의 컴파일러 인프라(예: LLVM IR)는 **하드웨어 독립적인 표현**에 강하지만, 다음과 같은 한계를 가지고 있었기 때문

### 1. **도메인 특화 연산 표현 부족**

- 예를 들어 딥러닝에서는 `matmul`, `convolution`, `softmax` 같은 고수준 연산이 중요함
- 하지만 LLVM IR은 이런 연산을 표현할 수단이 없고, 낮은 수준의 연산(load/store/add)으로만 표현됨
- 결과적으로 **연산의 의미를 잃게 되어 최적화가 어렵고**, **비효율적인 코드 생성**이 많았음

### 2. **다양한 수준의 중간 표현이 필요함**

- 예: 고수준 (텐서 연산) → 중간 수준 (루프 및 버퍼) → 저수준 (레지스터와 포인터)
- 기존 IR은 단일 수준만을 가정하므로 **연산 수준 간 정보 전달**이 어려움

### 3. **다양한 하드웨어 대응 어려움**

- GPU, TPU, FPGA 등 다양한 백엔드에 맞춰 코드를 생성하려면 IR 자체도 유연해야 함
- 기존에는 백엔드별로 따로 DSL/컴파일러를 만들어야 했고, **중복 코드와 생태계 분산**이 심각했음

### 4. **재사용 가능한 컴파일러 인프라 부족**

- DSL마다 전용 컴파일러를 만들다 보면, **IR, 최적화, lowering 로직이 중복**됨
- MLIR은 **공통 인프라**를 제공하여, DSL들이 **공통 언어 위에서 동작**하게 하려는 목표를 가짐